{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The cell below is for you to keep track of the libraries used and install those libraries quickly\n",
    "##### Ensure that the proper library names are used and the syntax of `%pip install PACKAGE_NAME` is followed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install pandas \n",
    "# %pip install matplotlib\n",
    "# %pip install seaborn\n",
    "# %pip install numpy\n",
    "# %pip install sklearn\n",
    "# %pip install imblearn\n",
    "# add commented pip installation lines for packages used as shown above for ease of testing\n",
    "# the line should be of the format %pip install PACKAGE_NAME "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **DO NOT CHANGE** the filepath variable\n",
    "##### Instead, create a folder named 'data' in your current working directory and \n",
    "##### have the .parquet file inside that. A relative path *must* be used when loading data into pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Can have as many cells as you want for code\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "filepath = \"./data/catB_train.parquet\" \n",
    "# the initialised filepath MUST be a relative path to a folder named data that contains the parquet file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **ALL** Code for machine learning and dataset analysis should be entered below. \n",
    "##### Ensure that your code is clear and readable.\n",
    "##### Comments and Markdown notes are advised to direct attention to pieces of code you deem useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet(filepath)\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Data Analysis (60%)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Data cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Drop columns that have 100% missing values\n",
    "Reason: Impossible to guess a column without any prexisting data for that column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_missing = df.columns[df.isnull().sum() > 0]\n",
    "cols_missing_all = df.columns[df.isnull().sum() == df.shape[0]]\n",
    "cols_missing_100 = cols_missing.intersection(cols_missing_all)\n",
    "df.drop(cols_missing_100, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Fill nan with zero\n",
    "\n",
    "['f_ever_declined_la', 'recency_lapse', 'flg_affconnect_show_interest_ever',\n",
    "'flg_affconnect_ready_to_buy_ever', 'clmcon_visit_days', 'hlthclaim_amt',\n",
    "'hlthclaim_cnt_success', 'hlthclaim_cnt_unsuccess', 'giclaim_amt', 'f_purchase_lh']<br/>\n",
    "Reason: Most of these are just binary fields, and it is logical to replace them with 0 if we see that only 1 is present\n",
    "Another reason: Some of these columns are related to amount claim or purchase, which we assume that 0 means it is not relavent for them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_missing_with_zero(df, column_name):\n",
    "    df[column_name].fillna(0, inplace=True)\n",
    "    return df\n",
    "\n",
    "\n",
    "columns_to_replace_missing_with_zero = ['f_ever_declined_la', 'recency_lapse', 'flg_affconnect_show_interest_ever',\n",
    "                                        'flg_affconnect_ready_to_buy_ever', 'clmcon_visit_days', 'hlthclaim_amt',\n",
    "                                        'hlthclaim_cnt_success', 'hlthclaim_cnt_unsuccess', 'giclaim_amt', 'f_purchase_lh']\n",
    "\n",
    "for i in columns_to_replace_missing_with_zero:\n",
    "    df = replace_missing_with_zero(df, i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Fill nan with one\n",
    "is_dependent_in_at_least_1_policy <br/>\n",
    "Reason: The distinct value for this col is 0, and the name suggests that it is a binary field, so we fill missing values with 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_missing_with_one(df, column_name):\n",
    "    df[column_name].fillna(1, inplace=True)\n",
    "    return df\n",
    "\n",
    "\n",
    "columns_to_replace_missing_with_one = ['is_dependent_in_at_least_1_policy']\n",
    "\n",
    "for i in columns_to_replace_missing_with_one:\n",
    "    df = replace_missing_with_one(df, i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Fill nan with median\n",
    "'hh_20', 'pop_20', 'hh_size', 'annual_income_est' <br/>\n",
    "Reason: They follow a roughly normal distribution data, so replacing with median seems logical here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill nan with median\n",
    "def replace_missing_with_median(df, column_name):\n",
    "    median_value = df[column_name].median()\n",
    "    df[column_name].fillna(median_value, inplace=True)\n",
    "    return df\n",
    "\n",
    "columns_to_replace_missing_with_median = [\n",
    "    'hh_20', 'pop_20', 'hh_size']\n",
    "\n",
    "for i in columns_to_replace_missing_with_median:\n",
    "    df = replace_missing_with_median(df, i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Fill missing values according to proportion of missing values in each column\n",
    "'race_desc', 'cltsex_fix' <br/>\n",
    "Reason: They only have a few missing values, so it is reasonable to fill up the missing ones according to the current data proportion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper Functions\n",
    "def value_proportions(arr):\n",
    "    unique_values, counts = np.unique(arr, return_counts=True)\n",
    "    total_count = arr.size\n",
    "    proportions = counts / total_count\n",
    "    return dict(zip(unique_values, proportions))\n",
    "\n",
    "\n",
    "def fill_missing_with_proportions(df, column_name):\n",
    "    if column_name not in df.columns:\n",
    "        raise ValueError(f\"Column '{column_name}' not found in DataFrame\")\n",
    "\n",
    "    # Get the non-missing values and their proportions\n",
    "    non_missing_values = df[column_name].dropna()\n",
    "    value_distribution = value_proportions(non_missing_values)\n",
    "\n",
    "    # Convert the value distribution into two lists for np.random.choice\n",
    "    values, probabilities = zip(*value_distribution.items())\n",
    "\n",
    "    # Count the missing values\n",
    "    missing_count = df[column_name].isna().sum()\n",
    "\n",
    "    if missing_count > 0:\n",
    "        # Generate random samples based on the value distribution\n",
    "        random_samples = np.random.choice(\n",
    "            values, size=missing_count, p=probabilities)\n",
    "\n",
    "        # Fill in the missing values with these random samples\n",
    "        df.loc[df[column_name].isna(), column_name] = random_samples\n",
    "\n",
    "    return df\n",
    "\n",
    "df = fill_missing_with_proportions(df, 'race_desc')\n",
    "df = fill_missing_with_proportions(df, 'cltsex_fix')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Delete rows with continuous missing values about flg_* columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper Functions\n",
    "def delete_rows_with_all_missing(df, columns_names):\n",
    "    # Initial number of rows\n",
    "    initial_row_count = len(df)\n",
    "\n",
    "    # Drop rows where all values in the selected columns are missing\n",
    "    df = df.dropna(subset=columns_names, how='all')\n",
    "\n",
    "    # Calculate the number of rows deleted\n",
    "    deleted_rows_count = initial_row_count - len(df)\n",
    "\n",
    "    print(f\"Deleted {deleted_rows_count} rows.\")\n",
    "\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "SELECTED_FLG_COLUMNS = ['flg_substandard', 'flg_is_borderline_standard', 'flg_is_revised_term',\n",
    "                        'flg_is_rental_flat', 'flg_has_health_claim', 'flg_has_life_claim',\n",
    "                        'flg_gi_claim', 'flg_is_proposal', 'flg_with_preauthorisation',\n",
    "                        'flg_is_returned_mail']\n",
    "\n",
    "df = delete_rows_with_all_missing(df, SELECTED_FLG_COLUMNS)\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shape after cleaning\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop col 'clntnum' <br/>\n",
    "Reason: This is just a unique identifier for each customer, which doesn't contribute much"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop('clntnum', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Drop columns that have no missing values and one unique value\n",
    "Reason: One  unique values doesnt really do much"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_no_missing = df.columns[df.isnull().sum() == 0]\n",
    "cols_one_unique = df.columns[df.nunique() == 1]\n",
    "cols_to_drop = cols_no_missing.intersection(cols_one_unique)\n",
    "df.drop(cols_to_drop, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "drop ['is_consent_to_mail', 'is_consent_to_email', 'is_consent_to_call', 'is_consent_to_sms', 'is_valid_dm', 'is_valid_email']<br/>\n",
    "Reason: These are related to communication, and we don't think it contributes much"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "COMMUNICATION_PREFERENCE_COLS = [\n",
    "    'is_consent_to_mail', 'is_consent_to_email', 'is_consent_to_call', 'is_consent_to_sms', 'is_valid_dm', 'is_valid_email'\n",
    "]\n",
    "\n",
    "# df.drop(COMMUNICATION_PREFERENCE_COLS, axis=1, inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "drop ctrycode_desc <br/>\n",
    "Reason: 99% of the data here is Singapore, which doesn't really contributes much information\n",
    "\n",
    "drop hh_size_est <br/>\n",
    "Reason: Already have hh_size, no need this\n",
    "\n",
    "drop 'recency_cancel', 'tot_cancel_pols' <br/>\n",
    "Reason: We believe that the information from recency lapse should be sufficient\n",
    "\n",
    "drop 'n_months_since_visit_affcon', 'recency_clmcon', 'recency_clmcon_regis', 'recency_hlthclaim', 'recency_hlthclaim_success', 'recency_hlthclaim_unsuccess', 'flg_hlthclaim_839f8a_ever', 'recency_hlthclaim_839f8a', 'flg_hlthclaim_14cb37_ever', 'recency_hlthclaim_14cb37', 'recency_giclaim'\n",
    "<br /><br />Reason: All of these have too many missing values, and also some like flg_hlthclaim_14cb37_ever is related to a specific insurance which we decide to not focus for now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DROP_COLS = [\n",
    "    'ctrycode_desc', 'hh_size_est', 'recency_cancel', 'tot_cancel_pols', 'flg_affconnect_lapse_ever', 'affcon_visit_days', 'n_months_since_visit_affcon', 'recency_clmcon', 'recency_clmcon_regis',\n",
    "    'recency_hlthclaim', 'recency_hlthclaim_success', 'recency_hlthclaim_unsuccess', 'flg_hlthclaim_839f8a_ever', 'recency_hlthclaim_839f8a', 'flg_hlthclaim_14cb37_ever', 'recency_hlthclaim_14cb37',\n",
    "    'recency_giclaim'\n",
    "]\n",
    "\n",
    "# Drop cols that are not needed\n",
    "df.drop(DROP_COLS, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "drop 'lapse_ape_*' <br/>\n",
    "Reason: We believe that the information from recency lapse should be sufficient\n",
    "\n",
    "drop n_months_since_lapse_ <br/>\n",
    "Reason: Since we drop all cols starting with 'lapse_ape_', all of n_months_since_lapse_ should be drop too since they relate to one another\n",
    "\n",
    "drop cols 'f_hold_*', 'f_ever_bought_*' <br/>\n",
    "Reason: Related to individual insurance that we decide not to focus for now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DROP_COLS_REGEX = ['lapse_ape_*', 'n_months_since_lapse_*',\n",
    "                   'f_hold_*', 'f_ever_bought_*']\n",
    "\n",
    "# Drop cols regex\n",
    "cols_to_drop = df.columns[df.columns.str.contains('|'.join(DROP_COLS_REGEX))]\n",
    "df.drop(cols_to_drop, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shape after feature selection\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Change 'annual_income_est' from string to integer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in df.iterrows():\n",
    "    if row['annual_income_est'] == 'A.ABOVE200K':\n",
    "        df.loc[index, 'annual_income_est'] = 200000\n",
    "    elif row['annual_income_est'] == 'B.100K-200K':\n",
    "        df.loc[index, 'annual_income_est'] = 150000\n",
    "    elif row['annual_income_est'] == 'C.60K-100K':\n",
    "        df.loc[index, 'annual_income_est'] = 80000\n",
    "    elif row['annual_income_est'] == 'D.30K-60K':\n",
    "        df.loc[index, 'annual_income_est'] = 45000\n",
    "    elif row['annual_income_est'] == 'E.BELOW30K':\n",
    "        df.loc[index, 'annual_income_est'] = 30000\n",
    "\n",
    "\n",
    "# Fill nan of annual_income_est with median\n",
    "df = replace_missing_with_median(df, 'annual_income_est')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming df is your DataFrame\n",
    "# (replace it with the actual DataFrame if it's defined elsewhere)\n",
    "\n",
    "# Print all cols that have less than 4 unique values put the columns in a list\n",
    "cols = ['clttype', 'stat_flag', 'cltsex_fix', 'flg_substandard',\n",
    "        'flg_is_borderline_standard', 'flg_is_revised_term',\n",
    "        'flg_is_rental_flat', 'flg_has_health_claim', 'flg_has_life_claim',\n",
    "        'flg_is_proposal', 'flg_with_preauthorisation',\n",
    "        'flg_is_returned_mail', 'is_housewife_retiree', 'is_sg_pr',\n",
    "        'is_class_1_2', 'f_ever_declined_la', 'flg_latest_being_lapse',\n",
    "        'flg_latest_being_cancel', 'sumins_grp_fe5fb8', 'f_elx', 'f_mindef_mha',\n",
    "        'f_retail', 'flg_affconnect_show_interest_ever',\n",
    "        'flg_affconnect_ready_to_buy_ever', 'is_consent_to_mail',\n",
    "        'is_consent_to_email', 'is_consent_to_call', 'is_consent_to_sms', 'is_valid_dm', 'is_valid_email']\n",
    "\n",
    "# Calculate the number of rows and columns for subplots dynamically\n",
    "num_cols = 1  # Display one column per row\n",
    "num_rows = len(cols)  # Number of rows\n",
    "\n",
    "# Iterate through each column\n",
    "for i, col in enumerate(cols):\n",
    "    prob_matrix = df.groupby([col, 'flg_gi_claim'])[\n",
    "        'f_purchase_lh'].mean().unstack()\n",
    "\n",
    "    # Create a new subplot for each column\n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "    # Plotting on the subplot\n",
    "    sns.heatmap(prob_matrix, annot=True, cmap=\"YlGnBu\", ax=ax, cbar=False)\n",
    "\n",
    "    # Set title and labels\n",
    "    ax.set_title(f'Heatmap for {col} vs flg_gi_claim')\n",
    "    ax.set_xlabel('flg_gi_claim')\n",
    "    ax.set_ylabel(col)\n",
    "\n",
    "    # Show the plot\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Print all cols that have less than 4 unique values put the columns in a list\n",
    "cols = ['clttype', 'stat_flag', 'cltsex_fix', 'flg_substandard',\n",
    "        'flg_is_borderline_standard', 'flg_is_revised_term',\n",
    "        'flg_is_rental_flat', 'flg_has_health_claim', 'flg_has_life_claim',\n",
    "        'flg_is_proposal', 'flg_with_preauthorisation',\n",
    "        'flg_is_returned_mail', 'is_housewife_retiree', 'is_sg_pr',\n",
    "        'is_class_1_2', 'f_ever_declined_la', 'flg_latest_being_lapse',\n",
    "        'flg_latest_being_cancel', 'sumins_grp_fe5fb8', 'f_elx', 'f_mindef_mha',\n",
    "        'f_retail', 'flg_affconnect_show_interest_ever',\n",
    "        'flg_affconnect_ready_to_buy_ever', 'is_consent_to_mail', \n",
    "        'is_consent_to_email', 'is_consent_to_call', 'is_consent_to_sms', 'is_valid_dm', 'is_valid_email']\n",
    "\n",
    "# Calculate the number of rows and columns for subplots dynamically\n",
    "num_cols = 6  # Number of columns in each row\n",
    "num_rows = (len(cols) // num_cols) + (len(cols) %\n",
    "                                      num_cols > 0)  # Number of rows\n",
    "\n",
    "# Create subplots\n",
    "fig, axes = plt.subplots(num_rows, num_cols, figsize=(24, 5*num_rows))\n",
    "\n",
    "# Flatten the axes array for easy iteration\n",
    "axes = axes.flatten()\n",
    "\n",
    "# Iterate through each column\n",
    "for i, col in enumerate(cols):\n",
    "    prob_matrix = df.groupby([col, 'flg_gi_claim'])[\n",
    "        'f_purchase_lh'].mean().unstack()\n",
    "\n",
    "    # Plotting on the i-th subplot\n",
    "    sns.heatmap(prob_matrix, annot=True, cmap=\"YlGnBu\", ax=axes[i], cbar=False)\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Do One Hot Encoding for categorical variables\n",
    "\n",
    "'clttype', 'stat_flag', 'race_desc', 'cltsex_fix' <br/>\n",
    "Reason: They are categorical columns with not many distinct values, so one hot encoding can be used here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper Functions\n",
    "def find_col_index(df, col_name):\n",
    "    return df.columns.get_loc(col_name)\n",
    "\n",
    "def one_hot_encode(df, col, start_position=0):\n",
    "    df_encoded = pd.get_dummies(df[col], prefix=col, dtype=int)\n",
    "    df.drop(col, axis=1, inplace=True)\n",
    "    df = pd.concat([df.iloc[:, :start_position], df_encoded,\n",
    "                   df.iloc[:, start_position:]], axis=1)\n",
    "    return df\n",
    "\n",
    "\n",
    "# One hot encoding for 'clttype'\n",
    "start_position = find_col_index(df, 'clttype')\n",
    "df = one_hot_encode(df, 'clttype', start_position)\n",
    "\n",
    "# One hot encoding for 'stat_flag'\n",
    "start_position = find_col_index(df, 'stat_flag')\n",
    "df = one_hot_encode(df, 'stat_flag', start_position)\n",
    "\n",
    "# One hot encoding for 'race_desc'\n",
    "start_position = find_col_index(df, 'race_desc')\n",
    "df = one_hot_encode(df, 'race_desc', start_position)\n",
    "\n",
    "# One hot encoding for 'cltsex_fix'\n",
    "start_position = find_col_index(df, 'cltsex_fix')\n",
    "df = one_hot_encode(df, 'cltsex_fix', start_position)\n",
    "\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Convert dates to years\n",
    "\n",
    "'min_occ_date', 'cltdob_fix' <br/>\n",
    "Reason: These are in dates, if we convert to years, they are only integers, which would be more compatible for the model to fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def years_until_2024(df, column_name, target_year=2024):\n",
    "    if column_name not in df.columns:\n",
    "        raise ValueError(f\"Column '{column_name}' not found in DataFrame\")\n",
    "\n",
    "    # Remove rows where the date column is \"None\"\n",
    "    df = df[df[column_name] != \"None\"]\n",
    "\n",
    "    # Strip the string of whitespace and convert to datetime using .loc\n",
    "    df.loc[:, column_name] = pd.to_datetime(\n",
    "        df[column_name].str.strip(), format='%Y-%m-%d', errors='raise')\n",
    "\n",
    "    # Calculate the number of years until 2024 using .loc\n",
    "    df.loc[:, column_name] = df[column_name].apply(\n",
    "        lambda x: target_year - x.year if pd.notnull(x) else None)\n",
    "\n",
    "    # Convert the column to integer data type\n",
    "    df = df.astype({column_name: 'int64'})\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "df = years_until_2024(df, 'min_occ_date', 2024)\n",
    "df = years_until_2024(df, 'cltdob_fix', 2024)\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Sum/Average columns of interest to create new column\n",
    "\n",
    "'ape_', 'sumins_', 'prempaid_', 'n_months_' <br/>\n",
    "Reason: Since they are related to various individual insurance, we decide to combine them for now with either summing or averaging(for months), to make it more simplified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper Functions\n",
    "def fetch_all_columns_with_prefix(df, prefix):\n",
    "    # Get all the columns that start with the prefix\n",
    "    cols_to_fetch = df.columns[df.columns.str.startswith(prefix)]\n",
    "\n",
    "    return cols_to_fetch.tolist()\n",
    "\n",
    "\n",
    "def sum_numeric_columns_prefix(df, prefix, new_column_name='sum', start_position=0):\n",
    "    # Get all the columns that start with the prefix\n",
    "    column_names = fetch_all_columns_with_prefix(df, prefix)\n",
    "\n",
    "    # Convert columns to numeric\n",
    "    for col in column_names:\n",
    "        if col in df.columns:\n",
    "            df[col] = pd.to_numeric(df[col], errors='raise')\n",
    "        else:\n",
    "            raise ValueError(f\"Column '{col}' not found in DataFrame\")\n",
    "\n",
    "    # Sum the columns\n",
    "    df[new_column_name] = df[column_names].sum(axis=1)\n",
    "\n",
    "    # Insert the new column at the specified start_position index\n",
    "    df = pd.concat([df.iloc[:, :start_position],\n",
    "                   df[new_column_name], df.iloc[:, start_position:-1]], axis=1)\n",
    "\n",
    "    # Drop the original columns\n",
    "    df = df.drop(column_names, axis=1)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def average_numeric_columns(df, prefix, new_columns_name='average', start_position=0):\n",
    "    # Get all the columns that start with the prefix\n",
    "    column_names = fetch_all_columns_with_prefix(df, prefix)\n",
    "\n",
    "    # Convert columns to numeric\n",
    "    for col in column_names:\n",
    "        if col in df.columns:\n",
    "            df[col] = pd.to_numeric(df[col], errors='raise')\n",
    "        else:\n",
    "            raise ValueError(f\"Column '{col}' not found in DataFrame\")\n",
    "\n",
    "    # Sum the columns\n",
    "    df[new_columns_name] = df[column_names].mean(axis=1).round(2)\n",
    "\n",
    "    # Insert the new column at the specified start_position index\n",
    "    df = pd.concat([df.iloc[:, :start_position],\n",
    "                   df[new_columns_name], df.iloc[:, start_position:-1]], axis=1)\n",
    "\n",
    "    df = df.drop(column_names, axis=1)\n",
    "\n",
    "    return df\n",
    "\n",
    "# Sum the columns\n",
    "start_position = find_col_index(\n",
    "    df, fetch_all_columns_with_prefix(df, 'ape_')[0])\n",
    "df = sum_numeric_columns_prefix(df, 'ape_', 'sum_ape', start_position)\n",
    "\n",
    "start_position = find_col_index(\n",
    "    df, fetch_all_columns_with_prefix(df, 'sumins_')[0])\n",
    "df = sum_numeric_columns_prefix(df, 'sumins_', 'sum_sumins', start_position)\n",
    "\n",
    "start_position = find_col_index(\n",
    "    df, fetch_all_columns_with_prefix(df, 'prempaid_')[0])\n",
    "df = sum_numeric_columns_prefix(\n",
    "    df, 'prempaid_', 'sum_prempaid', start_position)\n",
    "\n",
    "start_position = find_col_index(\n",
    "    df, fetch_all_columns_with_prefix(df, 'n_months_')[0])\n",
    "df = average_numeric_columns(df, 'n_months_', 'avg_n_months_', start_position)\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Check for columns that should be in int or float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print all columns that are not int or float\n",
    "print(\"Columns that are not int or float\")\n",
    "print(df.select_dtypes(exclude=['int', 'float']).columns)\n",
    "\n",
    "df = df.astype({'hh_20': 'int64', 'pop_20': 'int64',\n",
    "               'hlthclaim_amt': 'float', 'giclaim_amt': 'float'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Data Visualisation\n",
    "Clarity, Relevance, Clustering Techniques, Aesthetics and Insightfulness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print heatmap of correlation matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "plt.figure(figsize=(50, 50))\n",
    "sns.heatmap(df.corr(), annot=True, cmap='coolwarm')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Histogram for each column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_cols = len(df.columns)\n",
    "num_rows = (num_cols // 8) + 1\n",
    "\n",
    "# Create subplots\n",
    "fig, axes = plt.subplots(num_rows, 8, figsize=(40, num_rows*5), facecolor='white')\n",
    "fig.suptitle('Histograms for All Columns', y=1.01, fontsize=20)\n",
    "\n",
    "# Flatten the axes array\n",
    "axes = axes.flatten()\n",
    "\n",
    "# Plot histograms for each column\n",
    "for i, col in enumerate(df.columns):\n",
    "    axes[i].hist(df[col], bins=20, edgecolor='black', linewidth=1.2)\n",
    "    axes[i].set_title(col)\n",
    "    axes[i].set_xlabel('Value')\n",
    "    axes[i].set_ylabel('Frequency')\n",
    "\n",
    "for i in range(num_cols, num_rows * 8):\n",
    "    fig.delaxes(axes[i])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seperate the row with the target column = 0 and the row with the target column = 1\n",
    "df_0 = df[df['f_purchase_lh'] == 0]\n",
    "df_1 = df[df['f_purchase_lh'] == 1]\n",
    "num_cols = len(df_0.columns)\n",
    "num_rows = (num_cols // 8) + 1\n",
    "\n",
    "# Create subplots\n",
    "fig, axes = plt.subplots(num_rows, 8, figsize=(\n",
    "    40, num_rows*5), facecolor='white')\n",
    "fig.suptitle('Histograms for df_0 Columns', y=1.01, fontsize=20)\n",
    "\n",
    "# Flatten the axes array\n",
    "axes = axes.flatten()\n",
    "\n",
    "# Plot histograms for each column\n",
    "for i, col in enumerate(df_0.columns):\n",
    "    axes[i].hist(df_0[col], bins=20, edgecolor='black', linewidth=1.2)\n",
    "    axes[i].set_title(col)\n",
    "    axes[i].set_xlabel('Value')\n",
    "    axes[i].set_ylabel('Frequency')\n",
    "\n",
    "for i in range(num_cols, num_rows * 8):\n",
    "    fig.delaxes(axes[i])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_cols = len(df_1.columns)\n",
    "num_rows = (num_cols // 8) + 1\n",
    "\n",
    "# Create subplots\n",
    "fig, axes = plt.subplots(num_rows, 8, figsize=(\n",
    "    40, num_rows*5), facecolor='white')\n",
    "fig.suptitle('Histograms for df_1 Columns', y=1.01, fontsize=20)\n",
    "\n",
    "# Flatten the axes array\n",
    "axes = axes.flatten()\n",
    "\n",
    "# Plot histograms for each column\n",
    "for i, col in enumerate(df_1.columns):\n",
    "    axes[i].hist(df_1[col], bins=20, edgecolor='black', linewidth=1.2)\n",
    "    axes[i].set_title(col)\n",
    "    axes[i].set_xlabel('Value')\n",
    "    axes[i].set_ylabel('Frequency')\n",
    "\n",
    "for i in range(num_cols, num_rows * 8):\n",
    "    fig.delaxes(axes[i])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some Business Insights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1. Low Insurance Purchase in the Past 3 Months:\n",
    "The data suggests that the number of customers who have purchased insurance in the past 3 months is significantly lower than those who haven't. This might indicate a potential opportunity for Singlife to explore marketing strategies or incentives to encourage more customers to buy insurance within this time frame. Understanding the reasons behind this trend could help in designing targeted campaigns.\n",
    "\n",
    "##### 2. Most of the insurance bought are personal insurnace labeled as 'dttype_P':\n",
    "The majority of insurance purchases are categorized as personal insurance ('dttype_P'). This insight indicates a strong preference or demand for personal insurance among customers. Singlife could leverage this information to further tailor and enhance their personal insurance products to meet the specific needs and preferences of their customer base.\n",
    "\n",
    "##### 3. The proportion of Indian, Malay and others are much less than Chinese:\n",
    "The data reveals that the proportion of customers from Indian, Malay, and other ethnicities is significantly lower compared to Chinese customers. This could be an area of exploration for Singlife to diversify their customer base by understanding and addressing the specific needs and preferences of customers from different ethnic backgrounds.\n",
    "\n",
    "##### 4. The customer's age distribution is roughly normal, with the mean around 40, we can see that the customers below 30 are much less:\n",
    "The age distribution of customers appears to be roughly normal, with a mean around 40. However, there's a noticeable decline in the number of customers below 30. Singlife could consider developing products or marketing strategies that specifically target younger customers, as they represent a potential untapped market.\n",
    "\n",
    "##### 5. There is about a 50-50 split with customers who are in class 1 and 2 vs those who are not:\n",
    "There seems to be an approximately 50-50 split between customers in class 1 and 2 compared to those in other classes. While as students, we do not know what class 1 and 2 means here, but understanding the characteristics and preferences of customers in these classes can help Singlife tailor their products to better meet the needs of these specific customer segments.\n",
    "\n",
    "##### 6. Left Skewed Annual Income Distribution:\n",
    "The annual income distribution is highly skewed to the left, with a median below $50,000. This suggests that the majority of customers are not high-income earners. Singlife could design insurance products that are affordable and provide value to customers with lower incomes, potentially expanding their market reach.\n",
    "\n",
    "##### 7. Most of the customers only have 1 policy:\n",
    "Customers with in-force policies are predominantly those with 1 policy, and very few customers have more than 5 policies. Singlife could explore strategies to encourage existing customers to purchase additional policies, such as offering bundled products or discounts for multiple policies.\n",
    "\n",
    "##### 8. Most of the customers do not buy any insurance in the past 3 months:\n",
    "Reiterating the initial insight, the low number of customers who bought insurance in the past 3 months highlights a potential area for growth. Singlife could conduct further market research to understand the reasons behind this trend and design products or campaigns that address customer concerns and encourage them to purchase insurance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Business Insights:\n",
    "What is the differences between those who buy and those who don’t?\n",
    "Age: those who buy are generally younger, compared to the whole dataset\n",
    "Possible Explaination: Older population tend to already have existing insurance\n",
    "What it means: More insurance policies to target younger population / target older ones\n",
    "\n",
    "Income: Those who buy have higher annual incomes\n",
    "Possible Explaination: Higher income = more disposable income = more likely to buy insurance\n",
    "What it means: More insurance policies to target higher income / target lower income"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Remove columns that have <0.01 correlation with target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Select columns with correlation between -0.01 and 0.01 with 'f_purchase_lh'\n",
    "# selected_columns = df.columns[(\n",
    "#     df.corr()['f_purchase_lh'] > -0.01) & (df.corr()['f_purchase_lh'] < 0.01)].tolist()\n",
    "\n",
    "# print(selected_columns)\n",
    "\n",
    "# # Exclude 'f_purchase_lh' itself from the list if it's present\n",
    "# if 'f_purchase_lh' in selected_columns:\n",
    "#     selected_columns.remove('f_purchase_lh')\n",
    "\n",
    "# df = df.drop(selected_columns, axis=1)\n",
    "# df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import accuracy_score, f1_score, recall_score, precision_score\n",
    "\n",
    "\n",
    "def print_metrics(y_true, y_pred, dataset_name):\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    f1 = f1_score(y_true, y_pred, average='macro')\n",
    "    recall = recall_score(y_true, y_pred)\n",
    "    precision = precision_score(y_true, y_pred)\n",
    "\n",
    "    print(f\"{dataset_name} Set:\\t Accuracy: {accuracy:.2f},\\t F1: {f1:.2f}, Recall: {recall:.2f},\\t Precision: {precision:.2f}\")\n",
    "\n",
    "\n",
    "def print_confusion_matrix(y_true, y_pred, dataset_name):\n",
    "    matrix = confusion_matrix(y_true, y_pred)\n",
    "    sns.heatmap(matrix, annot=True, fmt='d', cmap='Blues', cbar=False)\n",
    "    plt.title(f'Confusion Matrix for {dataset_name} Set')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Sanity Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_int_float_values(df, include_list=['int', 'float']):\n",
    "    # Select only integer and float columns\n",
    "    selected_df = df.select_dtypes(include=include_list)\n",
    "\n",
    "    # Compare selected columns with original DataFrame columns\n",
    "    if set(selected_df.columns) != set(df.columns):\n",
    "        non_int_float_columns = set(df.columns) - set(selected_df.columns)\n",
    "        raise ValueError(\n",
    "            f\"Non-integer/float values found in columns: {', '.join(non_int_float_columns)}\")\n",
    "    else:\n",
    "        print(\"Success: All values in DataFrame are integers or floats.\")\n",
    "\n",
    "\n",
    "def check_missing_values(df):\n",
    "    # Find columns with missing values\n",
    "    missing_columns = df.columns[df.isna().any()].tolist()\n",
    "\n",
    "    # Check if there are any missing columns\n",
    "    if missing_columns:\n",
    "        print(f\"Missing values found in columns: {', '.join(missing_columns)}\")\n",
    "        raise ValueError(\n",
    "            f\"Missing values found in columns: {', '.join(missing_columns)}\")\n",
    "    else:\n",
    "        print(\"Success: No missing values found in DataFrame.\")\n",
    "\n",
    "\n",
    "check_int_float_values(df)\n",
    "check_missing_values(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check df shape\n",
    "print(f\"Number of rows: {df.shape[0]}\")\n",
    "print(f\"Number of columns: {df.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Modelling (30%)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Handling Imbalanced Data\n",
    "\n",
    "#### Technique used:\n",
    "SMOTE\n",
    "\n",
    "#### Effectiveness and Impact:\n",
    "This approach has proven empirically to improve the predictive performance of classifiers compared to undersampling the majority class or oversampling the minority class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import accuracy_score, f1_score, recall_score, precision_score\n",
    "\n",
    "\n",
    "def print_metrics(y_true, y_pred, dataset_name):\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    f1 = f1_score(y_true, y_pred, average='macro')\n",
    "    recall = recall_score(y_true, y_pred)\n",
    "    precision = precision_score(y_true, y_pred)\n",
    "\n",
    "    print(f\"{dataset_name} Set:\\t Accuracy: {accuracy:.2f},\\t F1: {f1:.2f}, Recall: {recall:.2f},\\t Precision: {precision:.2f}\")\n",
    "\n",
    "\n",
    "def print_confusion_matrix(y_true, y_pred, dataset_name):\n",
    "    matrix = confusion_matrix(y_true, y_pred)\n",
    "    sns.heatmap(matrix, annot=True, fmt='d', cmap='Blues', cbar=False)\n",
    "    plt.title(f'Confusion Matrix for {dataset_name} Set')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import numpy as np\n",
    "\n",
    "# Split data into train, dev, and test sets (e.g., 60%, 20%, 20%)\n",
    "\n",
    "df_decisions = df.copy()\n",
    "\n",
    "X = df_decisions.drop('f_purchase_lh', axis=1)\n",
    "y = df_decisions['f_purchase_lh']\n",
    "\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "X_dev, X_test, y_dev, y_test = train_test_split(X_temp, y_temp, test_size=0.4, random_state=42)\n",
    "\n",
    "smote = SMOTE(random_state=42, sampling_strategy=0.5)\n",
    "X_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)\n",
    "print(\"number of rows after SMOTE: \", X_train_smote.shape[0])\n",
    "y_train_smote.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/ms/9mlzst3s2vj4lsly34ghl8900000gn/T/ipykernel_49593/1892597156.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0;31m# Train the classifier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m         \u001b[0mclf_ri\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_ri\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_ri\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0mX_ri_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mselected_columns\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sklearn/base.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1349\u001b[0m                 )\n\u001b[1;32m   1350\u001b[0m             ):\n\u001b[0;32m-> 1351\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfit_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1352\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1353\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_forest.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0;31m# parallel_backend contexts set at a higher level,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m             \u001b[0;31m# since correctness does not rely on using threads.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             trees = Parallel(\n\u001b[0m\u001b[1;32m    490\u001b[0m                 \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m                 \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mdelayed_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m         )\n\u001b[0;32m---> 67\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterable_with_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1950\u001b[0m         \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1951\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1952\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturn_generator\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1953\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1954\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_get_outputs\u001b[0;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[1;32m   1593\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1594\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1595\u001b[0;31m                 \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_retrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1596\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1597\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mGeneratorExit\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_retrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1705\u001b[0m                 (self._jobs[0].get_status(\n\u001b[1;32m   1706\u001b[0m                     timeout=self.timeout) == TASK_PENDING)):\n\u001b[0;32m-> 1707\u001b[0;31m                 \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1708\u001b[0m                 \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1709\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "import random\n",
    "import itertools\n",
    "\n",
    "column_names = df.columns.tolist()\n",
    "column_names.remove('f_purchase_lh')\n",
    "# fixed_columns = ['flg_gi_claim']\n",
    "fixed_columns = []\n",
    "\n",
    "max_accuracy = 0\n",
    "max_f1 = 0\n",
    "current_best_columns = fixed_columns.copy()\n",
    "\n",
    "for num_columns in range(2, len(column_names) + 1):\n",
    "    # Generate combinations with fixed and current best columns\n",
    "    print(\"Number of columns: \", num_columns)\n",
    "    \n",
    "    # Remove currnet best columns from the list of columns\n",
    "    column_names = [col for col in column_names if col not in current_best_columns]\n",
    "    \n",
    "    column_combinations = [\n",
    "        combo for combo in itertools.combinations(column_names, num_columns - len(current_best_columns))\n",
    "    ]\n",
    "\n",
    "    # Add the current best columns to each combination\n",
    "    column_combinations = [\n",
    "        tuple(list(current_best_columns) + list(additional_columns))\n",
    "        for additional_columns in column_combinations\n",
    "    ]\n",
    "\n",
    "    # Iterate through combinations\n",
    "    for combination in column_combinations:\n",
    "        # Select the corresponding columns from the DataFrame\n",
    "        selected_columns = list(combination)\n",
    "        X_ri = X_train_smote[selected_columns]\n",
    "        y_ri = y_train_smote\n",
    "\n",
    "        clf_ri = RandomForestClassifier(\n",
    "            n_estimators=30, random_state=42, max_depth=num_columns // 2, min_samples_leaf=20, n_jobs=-1)\n",
    "\n",
    "        # Train the classifier\n",
    "        clf_ri.fit(X_ri, y_ri)\n",
    "\n",
    "        X_ri_test = X_test[selected_columns]\n",
    "\n",
    "        # Predict on the test set\n",
    "        y_ri_test_pred = clf_ri.predict(X_ri_test)\n",
    "\n",
    "        accuracy_test = accuracy_score(y_test, y_ri_test_pred)\n",
    "        f1_test = f1_score(y_test, y_ri_test_pred, average='macro')\n",
    "\n",
    "        if f1_test > max_f1:\n",
    "            max_accuracy = accuracy_test\n",
    "            max_f1 = f1_test\n",
    "            current_best_columns = combination\n",
    "            best_clf = clf_ri\n",
    "            best_y_test_pred = y_ri_test_pred\n",
    "            print(\"New Best! Max Accuracy: \", max_accuracy, \"Max F1: \", max_f1, \"Columns:\", current_best_columns)\n",
    "    \n",
    "print(\"Best Columns: \", current_best_columns, \"Max Accuracy: \",\n",
    "      max_accuracy, \"Max F1: \", max_f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Create a Decision Tree Classifier\n",
    "dt_classifier = DecisionTreeClassifier(random_state=42, max_depth=3)\n",
    "\n",
    "# Train the classifier\n",
    "dt_classifier.fit(X_train_smote, y_train_smote)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = dt_classifier.predict(X_test)\n",
    "y_train_pred = dt_classifier.predict(X_train)\n",
    "\n",
    "# Calculate the accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "\n",
    "\n",
    "print_metrics(y_train.tolist(), y_train_pred.tolist(), \"train\")\n",
    "print_metrics(y_test.tolist(), y_pred.tolist(), \"test\")\n",
    "print_confusion_matrix(y_test.tolist(), y_pred.tolist(), \"test\")\n",
    "print_confusion_matrix(y_train.tolist(), y_train_pred.tolist(), \"train\")\n",
    "\n",
    "\n",
    "# Print the depth of the Decision Tree\n",
    "print(f\"Depth of the Decision Tree: {dt_classifier.tree_.max_depth}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Create a Random Forest Classifier\n",
    "clf = RandomForestClassifier(random_state=42, max_depth=5)\n",
    "# Train the classifier\n",
    "clf.fit(X_train_smote, y_train_smote)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred_train_smote = clf.predict(X_train_smote)\n",
    "y_pred_dev = clf.predict(X_dev)\n",
    "y_pred_test = clf.predict(X_test)\n",
    "\n",
    "print_metrics(y_train.tolist(), y_train_pred.tolist(), \"train\")\n",
    "print_metrics(y_dev.tolist(), y_pred_dev.tolist(), \"dev\")\n",
    "print_metrics(y_test.tolist(), y_pred.tolist(), \"test\")\n",
    "print_confusion_matrix(y_train_smote.tolist(), y_pred_train_smote.tolist(), \"train\")\n",
    "print_confusion_matrix(y_test.tolist(), y_pred_test.tolist(), \"test\")\n",
    "\n",
    "\n",
    "for i, tree in enumerate(clf.estimators_):\n",
    "    print(f\"Tree {i} depth: {tree.tree_.max_depth}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate the importance of each feature by impurity decrease in random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the feature importances\n",
    "importances = clf.feature_importances_\n",
    "indices = np.argsort(importances)[::-1]\n",
    "feature_names = df.columns.tolist()\n",
    "print(\"Feature ranking:\")\n",
    "for f in range(X.shape[1]):\n",
    "    print(f\"{f + 1}. feature {feature_names[indices[f]]} ({importances[indices[f]]})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature importances\n",
    "importances = clf.feature_importances_\n",
    "indices = np.argsort(importances)[::-1]\n",
    "\n",
    "# Convert the importances into a Series for Seaborn\n",
    "importances_series = pd.Series(importances[indices], index=np.array(feature_names)[indices])\n",
    "\n",
    "# Plot the feature importances\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x=importances_series.index, y=importances_series.values, palette=\"Blues_d\")\n",
    "\n",
    "# Rotate feature names\n",
    "plt.xticks(rotation=45, ha=\"right\")\n",
    "plt.title(\"Feature Importances in Random Forest\")\n",
    "plt.ylabel(\"Importance\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Preprocessing\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import f1_score, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.decomposition import PCA\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Data Precessing: normalisaiton and convertion to pytorch tensor\n",
    "scaler = StandardScaler()\n",
    "pca = PCA(n_components=3)\n",
    "\n",
    "X_train_smote = X_train_smote[['min_occ_date', 'flg_gi_claim', 'is_valid_dm',\n",
    "                               'avg_n_months_', 'sum_sumins', 'f_retail', 'flg_affconnect_show_interest_ever']]\n",
    "X_dev = X_dev[['min_occ_date', 'flg_gi_claim', 'is_valid_dm', 'avg_n_months_',\n",
    "               'sum_sumins', 'f_retail', 'flg_affconnect_show_interest_ever']]\n",
    "X_test = X_test[['min_occ_date', 'flg_gi_claim', 'is_valid_dm', 'avg_n_months_',\n",
    "                 'sum_sumins', 'f_retail', 'flg_affconnect_show_interest_ever']]\n",
    "\n",
    "x_train_smote_scaled = scaler.fit_transform(X_train_smote)\n",
    "x_dev_scaled = scaler.transform(X_dev)\n",
    "x_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# X_train_smote_pca = pca.fit_transform(x_train_smote_scaled)\n",
    "# X_dev_pca = pca.transform(x_dev_scaled)\n",
    "# X_test_pca = pca.transform(x_test_scaled)\n",
    "\n",
    "X_train_smote_pca = x_train_smote_scaled\n",
    "X_dev_pca = x_dev_scaled\n",
    "X_test_pca = x_test_scaled\n",
    "\n",
    "x_train_tensor = torch.tensor(X_train_smote_pca, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train_smote.values, dtype=torch.long)\n",
    "train_dataset = TensorDataset(x_train_tensor, y_train_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "x_dev_tensor = torch.tensor(X_dev_pca, dtype=torch.float32)\n",
    "y_dev_tensor = torch.tensor(y_dev.values, dtype=torch.long)\n",
    "x_test_tensor = torch.tensor(X_test_pca, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test.values, dtype=torch.long)\n",
    "\n",
    "# Define Neural Network\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size_1=32, hidden_size_2=16, num_classes=2, dropout_rate=0.5):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(input_size, hidden_size_1),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.BatchNorm1d(hidden_size_1),\n",
    "            nn.Dropout(dropout_rate),\n",
    "\n",
    "            nn.Linear(hidden_size_1, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear_relu_stack(x)\n",
    "\n",
    "\n",
    "# Create the model\n",
    "input_size = X_train_smote_pca.shape[1]\n",
    "\n",
    "# Modify this based on your y_train shape\n",
    "num_classes = y_train_tensor.shape[0]\n",
    "model = NeuralNetwork(input_size, num_classes)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "num_epochs = 15  # minus one\n",
    "train_losses = []\n",
    "dev_losses = []\n",
    "test_losses = []\n",
    "train_f1_scores = []\n",
    "dev_f1_scores = []\n",
    "test_f1_scores = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for inputs, targets in train_loader:\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "\n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    model.eval()\n",
    "    # Calculate F1 score\n",
    "    with torch.no_grad():\n",
    "        # Training metrics\n",
    "        outputs = model(x_train_tensor)\n",
    "        train_loss = criterion(outputs, y_train_tensor).item()\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        train_f1 = f1_score(y_train_tensor, predicted, average='macro')\n",
    "        train_losses.append(train_loss)\n",
    "        train_f1_scores.append(train_f1)\n",
    "\n",
    "        # Development metrics\n",
    "        outputs = model(x_dev_tensor)\n",
    "        dev_loss = criterion(outputs, y_dev_tensor).item()\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        dev_f1 = f1_score(y_dev_tensor, predicted, average='macro')\n",
    "        dev_losses.append(dev_loss)\n",
    "        dev_f1_scores.append(dev_f1)\n",
    "\n",
    "        # Test metrics\n",
    "        outputs = model(x_test_tensor)\n",
    "        test_loss = criterion(outputs, y_test_tensor).item()\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        test_f1 = f1_score(y_test_tensor, predicted, average='macro')\n",
    "        test_losses.append(test_loss)\n",
    "        test_f1_scores.append(test_f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## save the model \n",
    "# torch.save(model, 'trained_model.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Choice of Model\n",
    "\n",
    "### 1. **Model Fit and Justification:**\n",
    "\n",
    "- **Feature Interactions:** FCNNs are well-suited for tasks where complex interactions between features need to be learned. In the provided dataset, there are various client-related features like demographic information, policy history, and claim metrics. A FCNN can effectively capture intricate relationships among these features which might be crucial for predicting future insurance purchases.\n",
    "\n",
    "- **Non-Linearity:** The data likely contains non-linear relationships, especially in variables like income, policy history, or demographic features. FCNNs, with their non-linear activation functions, are adept at modeling these non-linearities.\n",
    "\n",
    "- **Versatility and Adaptability:** FCNNs are known for their adaptability to a wide range of data types and structures. This makes them a good candidate for datasets with a mix of categorical and numerical data, as is the case here.\n",
    "\n",
    "### 2. **Complexity vs Performance:**\n",
    "\n",
    "- **Balance Between Complexity and Performance:** FCNNs offer a good balance between model complexity and performance. They are less complex and computationally expensive compared to other deep learning models like convolutional neural networks (CNNs) or recurrent neural networks (RNNs), especially when dealing with tabular data.\n",
    "\n",
    "- **Scalability:** If the dataset size increases, FCNNs can be scaled up by adding more layers or neurons, allowing them to handle more complex patterns without a significant increase in computational requirements.\n",
    "\n",
    "- **Overfitting Control:** With techniques like dropout and regularization, FCNNs can be prevented from overfitting, which is crucial given the diverse and possibly high-dimensional nature of the dataset.\n",
    "\n",
    "### 3. **Data Suitability:**\n",
    "\n",
    "- **Tabular Data:** FCNNs are particularly well-suited for tabular data, which seems to be the format of the Singlife dataset. They can efficiently process and learn from the structured format of such data.\n",
    "\n",
    "- **High Dimensionality:** The dataset includes a wide range of variables, indicating high dimensionality. FCNNs are capable of handling high-dimensional data, especially when techniques like dimensionality reduction (e.g., PCA) are applied as a preprocessing step.\n",
    "\n",
    "- **Diverse Feature Types:** The dataset contains a mix of continuous, categorical, and binary variables. FCNNs can handle this diversity by learning appropriate representations for different types of data.\n",
    "\n",
    "### 4. **General Considerations:**\n",
    "\n",
    "- **Rapid Development and Deployment:** The availability of robust frameworks and libraries for neural networks facilitates rapid development, testing, and deployment of FCNNs, accelerating the move from model development to application."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Performance (10%)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Model Performance Evaluation\n",
    "Precision Metric, Recall Metric, Balance and Context Relevance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluating\n",
    "plt.figure(figsize=(12, 10))\n",
    "\n",
    "# Plot Loss\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(range(num_epochs), train_losses, label='Training Loss')\n",
    "plt.plot(range(num_epochs), dev_losses, label='Dev Loss')\n",
    "plt.plot(range(num_epochs), test_losses, label='Test Loss')\n",
    "plt.title('Epoch vs Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# Plot F1 Score\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(range(num_epochs), train_f1_scores, label='Training F1 Score')\n",
    "plt.plot(range(num_epochs), dev_f1_scores, label='Dev F1 Score')\n",
    "plt.plot(range(num_epochs), test_f1_scores, label='Test F1 Score')\n",
    "plt.title('Epoch vs Macro F1 Score')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('F1 Score')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "# Function to calculate and plot confusion matrix\n",
    "def evaluate_and_plot_confusion_matrix(model, x_tensor, y_tensor, title):\n",
    "    with torch.no_grad():\n",
    "        outputs = model(x_tensor)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        print_confusion_matrix(y_tensor.numpy(), predicted.numpy(), title)\n",
    "\n",
    "# Function to calculate and print metrics\n",
    "def evaluate_metric(model, x_tensor, y_tensor, title):\n",
    "    with torch.no_grad():\n",
    "        outputs = model(x_tensor)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        print_metrics(y_tensor.numpy(), predicted.numpy(), title)\n",
    "\n",
    "\n",
    "# Use these functions in your code\n",
    "evaluate_metric(model, x_train_tensor, y_train_tensor, 'Training Set')\n",
    "evaluate_metric(model, x_dev_tensor, y_dev_tensor, 'Development Data')\n",
    "evaluate_metric(model, x_test_tensor, y_test_tensor, 'Test Data')\n",
    "\n",
    "evaluate_and_plot_confusion_matrix(\n",
    "    model, x_train_tensor, y_train_tensor, 'Training Set')\n",
    "evaluate_and_plot_confusion_matrix(\n",
    "    model, x_dev_tensor, y_dev_tensor, 'Development Data')\n",
    "evaluate_and_plot_confusion_matrix(\n",
    "    model, x_test_tensor, y_test_tensor, 'Test Data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ----- End of Datathon ----- #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The cell below is **NOT** to be removed\n",
    "##### The function is to be amended so that it accepts the given input (dataframe) and returns the required output (list). \n",
    "##### It is recommended to test the function out prior to submission\n",
    "-------------------------------------------------------------------------------------------------------------------------------\n",
    "##### The hidden_data parsed into the function below will have the same layout columns wise as the dataset *SENT* to you\n",
    "##### Thus, ensure that steps taken to modify the initial dataset to fit into the model are also carried out in the function below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Helper Functions for final submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def replace_missing_with_zero(df, column_name):\n",
    "    df[column_name].fillna(0, inplace=True)\n",
    "    return df\n",
    "\n",
    "\n",
    "def replace_missing_with_one(df, column_name):\n",
    "    df[column_name].fillna(1, inplace=True)\n",
    "    return df\n",
    "\n",
    "\n",
    "def replace_missing_with_median(df, column_name):\n",
    "    median_value = df[column_name].median()\n",
    "    df[column_name].fillna(median_value, inplace=True)\n",
    "    return df\n",
    "\n",
    "\n",
    "def value_proportions(arr):\n",
    "    unique_values, counts = np.unique(arr, return_counts=True)\n",
    "    total_count = arr.size\n",
    "    proportions = counts / total_count\n",
    "    return dict(zip(unique_values, proportions))\n",
    "\n",
    "\n",
    "def fill_missing_with_proportions(df, column_name):\n",
    "    if column_name not in df.columns:\n",
    "        raise ValueError(f\"Column '{column_name}' not found in DataFrame\")\n",
    "\n",
    "    # Get the non-missing values and their proportions\n",
    "    non_missing_values = df[column_name].dropna()\n",
    "    value_distribution = value_proportions(non_missing_values)\n",
    "\n",
    "    # Convert the value distribution into two lists for np.random.choice\n",
    "    values, probabilities = zip(*value_distribution.items())\n",
    "\n",
    "    # Count the missing values\n",
    "    missing_count = df[column_name].isna().sum()\n",
    "\n",
    "    if missing_count > 0:\n",
    "        # Generate random samples based on the value distribution\n",
    "        random_samples = np.random.choice(\n",
    "            values, size=missing_count, p=probabilities)\n",
    "\n",
    "        # Fill in the missing values with these random samples\n",
    "        df.loc[df[column_name].isna(), column_name] = random_samples\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def delete_rows_with_all_missing(df, columns_names):\n",
    "    # Initial number of rows\n",
    "    initial_row_count = len(df)\n",
    "\n",
    "    # Drop rows where all values in the selected columns are missing\n",
    "    df = df.dropna(subset=columns_names, how='all')\n",
    "\n",
    "    # Calculate the number of rows deleted\n",
    "    deleted_rows_count = initial_row_count - len(df)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def find_col_index(df, col_name):\n",
    "    return df.columns.get_loc(col_name)\n",
    "\n",
    "\n",
    "def one_hot_encode(df, col, start_position=0):\n",
    "    df_encoded = pd.get_dummies(df[col], prefix=col, dtype=int)\n",
    "    df.drop(col, axis=1, inplace=True)\n",
    "    df = pd.concat([df.iloc[:, :start_position], df_encoded,\n",
    "                   df.iloc[:, start_position:]], axis=1)\n",
    "    return df\n",
    "\n",
    "\n",
    "def years_until_2024(df, column_name, target_year=2024):\n",
    "    if column_name not in df.columns:\n",
    "        raise ValueError(f\"Column '{column_name}' not found in DataFrame\")\n",
    "\n",
    "    # Remove rows where the date column is \"None\"\n",
    "    df = df[df[column_name] != \"None\"]\n",
    "\n",
    "    # Strip the string of whitespace and convert to datetime using .loc\n",
    "    df.loc[:, column_name] = pd.to_datetime(\n",
    "        df[column_name].str.strip(), format='%Y-%m-%d', errors='raise')\n",
    "\n",
    "    # Calculate the number of years until 2024 using .loc\n",
    "    df.loc[:, column_name] = df[column_name].apply(\n",
    "        lambda x: target_year - x.year if pd.notnull(x) else None)\n",
    "\n",
    "    # Convert the column to integer data type\n",
    "    df = df.astype({column_name: 'int64'})\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def fetch_all_columns_with_prefix(df, prefix):\n",
    "    # Get all the columns that start with the prefix\n",
    "    cols_to_fetch = df.columns[df.columns.str.startswith(prefix)]\n",
    "\n",
    "    return cols_to_fetch.tolist()\n",
    "\n",
    "\n",
    "def sum_numeric_columns_prefix(df, prefix, new_column_name='sum', start_position=0):\n",
    "    # Get all the columns that start with the prefix\n",
    "    column_names = fetch_all_columns_with_prefix(df, prefix)\n",
    "\n",
    "    # Convert columns to numeric\n",
    "    for col in column_names:\n",
    "        if col in df.columns:\n",
    "            df[col] = pd.to_numeric(df[col], errors='raise')\n",
    "        else:\n",
    "            raise ValueError(f\"Column '{col}' not found in DataFrame\")\n",
    "\n",
    "    # Sum the columns\n",
    "    df[new_column_name] = df[column_names].sum(axis=1)\n",
    "\n",
    "    # Insert the new column at the specified start_position index\n",
    "    df = pd.concat([df.iloc[:, :start_position],\n",
    "                   df[new_column_name], df.iloc[:, start_position:-1]], axis=1)\n",
    "\n",
    "    # Drop the original columns\n",
    "    df = df.drop(column_names, axis=1)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def average_numeric_columns(df, prefix, new_columns_name='average', start_position=0):\n",
    "    # Get all the columns that start with the prefix\n",
    "    column_names = fetch_all_columns_with_prefix(df, prefix)\n",
    "\n",
    "    # Convert columns to numeric\n",
    "    for col in column_names:\n",
    "        if col in df.columns:\n",
    "            df[col] = pd.to_numeric(df[col], errors='raise')\n",
    "        else:\n",
    "            raise ValueError(f\"Column '{col}' not found in DataFrame\")\n",
    "\n",
    "    # Sum the columns\n",
    "    df[new_columns_name] = df[column_names].mean(axis=1).round(2)\n",
    "\n",
    "    # Insert the new column at the specified start_position index\n",
    "    df = pd.concat([df.iloc[:, :start_position],\n",
    "                   df[new_columns_name], df.iloc[:, start_position:-1]], axis=1)\n",
    "\n",
    "    df = df.drop(column_names, axis=1)\n",
    "\n",
    "    return df\n",
    "\n",
    "# --- End of Micro Helper Functions ---\n",
    "\n",
    "\n",
    "def data_cleaning(df):\n",
    "    df = df.copy()\n",
    "    # --- Drop 100% missing columns ---\n",
    "    cols_missing = df.columns[df.isnull().sum() > 0]\n",
    "    cols_missing_all = df.columns[df.isnull().sum() == df.shape[0]]\n",
    "    cols_missing_100 = cols_missing.intersection(cols_missing_all)\n",
    "    df.drop(cols_missing_100, axis=1, inplace=True)\n",
    "\n",
    "    # --- Replace missing values with 0 ---\n",
    "    columns_to_replace_missing_with_zero = ['f_ever_declined_la', 'recency_lapse', 'flg_affconnect_show_interest_ever',\n",
    "                                            'flg_affconnect_ready_to_buy_ever', 'clmcon_visit_days', 'hlthclaim_amt',\n",
    "                                            'hlthclaim_cnt_success', 'hlthclaim_cnt_unsuccess', 'giclaim_amt']\n",
    "\n",
    "    for i in columns_to_replace_missing_with_zero:\n",
    "        df = replace_missing_with_zero(df, i)\n",
    "\n",
    "    # --- Replace missing values with 1 ---\n",
    "    columns_to_replace_missing_with_one = ['is_dependent_in_at_least_1_policy']\n",
    "\n",
    "    for i in columns_to_replace_missing_with_one:\n",
    "        df = replace_missing_with_one(df, i)\n",
    "\n",
    "    # --- Replace missing values with median ---\n",
    "    columns_to_replace_missing_with_median = [\n",
    "        'hh_20', 'pop_20', 'hh_size']\n",
    "\n",
    "    for i in columns_to_replace_missing_with_median:\n",
    "        df = replace_missing_with_median(df, i)\n",
    "\n",
    "    # --- Fill missing values according to proportion of missing values in each column ---\n",
    "    df = fill_missing_with_proportions(df, 'race_desc')\n",
    "    df = fill_missing_with_proportions(df, 'cltsex_fix')\n",
    "\n",
    "    # --- Delete rows with continuous missing values about flg_* columns ---\n",
    "    SELECTED_FLG_COLUMNS = ['flg_substandard', 'flg_is_borderline_standard', 'flg_is_revised_term',\n",
    "                            'flg_is_rental_flat', 'flg_has_health_claim', 'flg_has_life_claim',\n",
    "                            'flg_gi_claim', 'flg_is_proposal', 'flg_with_preauthorisation',\n",
    "                            'flg_is_returned_mail']\n",
    "    df = delete_rows_with_all_missing(df, SELECTED_FLG_COLUMNS)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def feature_selection(df):\n",
    "    df = df.copy()\n",
    "    # --- Drop 'clntnum' column ---\n",
    "    df.drop('clntnum', axis=1, inplace=True)\n",
    "\n",
    "    # --- Drop columns that have no missing values and one unique value ---\n",
    "    cols_no_missing = df.columns[df.isnull().sum() == 0]\n",
    "    cols_one_unique = df.columns[df.nunique() == 1]\n",
    "    cols_to_drop = cols_no_missing.intersection(cols_one_unique)\n",
    "    df.drop(cols_to_drop, axis=1, inplace=True)\n",
    "\n",
    "    # --- Drop more cols ---\n",
    "    COMMUNICATION_PREFERENCE_COLS = [\n",
    "        'is_consent_to_mail', 'is_consent_to_email', 'is_consent_to_call', 'is_consent_to_sms', 'is_valid_dm', 'is_valid_email'\n",
    "    ]\n",
    "    DROP_COLS = [\n",
    "        'ctrycode_desc', *COMMUNICATION_PREFERENCE_COLS, 'hh_size_est', 'recency_cancel', 'tot_cancel_pols', 'flg_affconnect_lapse_ever', 'affcon_visit_days', 'n_months_since_visit_affcon', 'recency_clmcon', 'recency_clmcon_regis',\n",
    "        'recency_hlthclaim', 'recency_hlthclaim_success', 'recency_hlthclaim_unsuccess', 'flg_hlthclaim_839f8a_ever', 'recency_hlthclaim_839f8a', 'flg_hlthclaim_14cb37_ever', 'recency_hlthclaim_14cb37',\n",
    "        'recency_giclaim'\n",
    "    ]\n",
    "    DROP_COLS_REGEX = ['lapse_ape_*', 'n_months_since_lapse_*',\n",
    "                       'f_hold_*', 'f_ever_bought_*']\n",
    "\n",
    "    df.drop(DROP_COLS, axis=1, inplace=True)\n",
    "    cols_to_drop = df.columns[df.columns.str.contains(\n",
    "        '|'.join(DROP_COLS_REGEX))]\n",
    "    df.drop(cols_to_drop, axis=1, inplace=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def feature_engineering(df):\n",
    "    df = df.copy()\n",
    "    # --- Change 'annual_income_est' from string to integer ---\n",
    "    for index, row in df.iterrows():\n",
    "        if row['annual_income_est'] == 'A.ABOVE200K':\n",
    "            df.loc[index, 'annual_income_est'] = 200000\n",
    "        elif row['annual_income_est'] == 'B.100K-200K':\n",
    "            df.loc[index, 'annual_income_est'] = 150000\n",
    "        elif row['annual_income_est'] == 'C.60K-100K':\n",
    "            df.loc[index, 'annual_income_est'] = 80000\n",
    "        elif row['annual_income_est'] == 'D.30K-60K':\n",
    "            df.loc[index, 'annual_income_est'] = 45000\n",
    "        elif row['annual_income_est'] == 'E.BELOW30K':\n",
    "            df.loc[index, 'annual_income_est'] = 30000\n",
    "\n",
    "    # Fill nan of annual_income_est with median\n",
    "    df = replace_missing_with_median(df, 'annual_income_est')\n",
    "\n",
    "    # --- Do One Hot Encoding for categorical variables ---\n",
    "    # One hot encoding for 'clttype'\n",
    "    start_position = find_col_index(df, 'clttype')\n",
    "    df = one_hot_encode(df, 'clttype', start_position)\n",
    "\n",
    "    # One hot encoding for 'stat_flag'\n",
    "    start_position = find_col_index(df, 'stat_flag')\n",
    "    df = one_hot_encode(df, 'stat_flag', start_position)\n",
    "\n",
    "    # One hot encoding for 'race_desc'\n",
    "    start_position = find_col_index(df, 'race_desc')\n",
    "    df = one_hot_encode(df, 'race_desc', start_position)\n",
    "\n",
    "    # One hot encoding for 'cltsex_fix'\n",
    "    start_position = find_col_index(df, 'cltsex_fix')\n",
    "    df = one_hot_encode(df, 'cltsex_fix', start_position)\n",
    "\n",
    "    # --- Convert dates to years ---\n",
    "    df = years_until_2024(df, 'min_occ_date', 2024)\n",
    "    df = years_until_2024(df, 'cltdob_fix', 2024)\n",
    "\n",
    "    # --- Sum/Average columns of interest to create new columns ---\n",
    "    start_position = find_col_index(\n",
    "        df, fetch_all_columns_with_prefix(df, 'ape_')[0])\n",
    "    df = sum_numeric_columns_prefix(df, 'ape_', 'sum_ape', start_position)\n",
    "\n",
    "    start_position = find_col_index(\n",
    "        df, fetch_all_columns_with_prefix(df, 'sumins_')[0])\n",
    "    df = sum_numeric_columns_prefix(\n",
    "        df, 'sumins_', 'sum_sumins', start_position)\n",
    "\n",
    "    start_position = find_col_index(\n",
    "        df, fetch_all_columns_with_prefix(df, 'prempaid_')[0])\n",
    "    df = sum_numeric_columns_prefix(\n",
    "        df, 'prempaid_', 'sum_prempaid', start_position)\n",
    "\n",
    "    start_position = find_col_index(\n",
    "        df, fetch_all_columns_with_prefix(df, 'n_months_')[0])\n",
    "    df = average_numeric_columns(\n",
    "        df, 'n_months_', 'avg_n_months_', start_position)\n",
    "\n",
    "    # --- Check for columns that should be in int or float ---\n",
    "    df = df.astype({'hh_20': 'int64', 'pop_20': 'int64',\n",
    "                    'hlthclaim_amt': 'float', 'giclaim_amt': 'float'})\n",
    "\n",
    "    # --- Remove columns that have <0.01 correlation with target variable ---\n",
    "    columns_less = ['clttype_C', 'stat_flag_MATURED', 'flg_is_revised_term', 'flg_has_life_claim',\n",
    "                    'flg_with_preauthorisation', 'recency_lapse', 'clmcon_visit_days', 'hlthclaim_cnt_success']\n",
    "\n",
    "    df.drop(columns_less, axis=1, inplace=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def preprocessing_data_analysis(df):\n",
    "    df = df.copy()\n",
    "    # --- Data cleaning ---\n",
    "    df = data_cleaning(df)\n",
    "    print(\"After data cleaning\", df.shape)\n",
    "\n",
    "    # --- Feature selection ---\n",
    "    df = feature_selection(df)\n",
    "    print(\"After feature selection\", df.shape)\n",
    "\n",
    "    # --- Feature engineering ---\n",
    "    df = feature_engineering(df)\n",
    "    print(\"After feature engineering\", df.shape)\n",
    "\n",
    "    return df\n",
    "\n",
    "def preprocessing_model(hidden_data, scalar=scaler, pca=pca):\n",
    "    x_test_hidden_data = scalar.transform(hidden_data)\n",
    "    x_test_hidden_data = pca.transform(x_test_hidden_data)\n",
    "    x_test_hidden_data = torch.tensor(x_test_hidden_data, dtype=torch.float32)\n",
    "    \n",
    "    print(\"After scaling and PCA, before feeding into the model\", x_test_hidden_data.shape)\n",
    "    return x_test_hidden_data\n",
    "\n",
    "def predict_output(x_test_hidden_data, model=model):\n",
    "    with torch.no_grad():\n",
    "        # Training metric\n",
    "        return_y = model(x_test_hidden_data)\n",
    "        _, predicted = torch.max(return_y.data, 1)\n",
    "        return predicted.tolist()\n",
    "    \n",
    "def load_model(filepath):\n",
    "    model = torch.load(filepath)\n",
    "    model.eval()\n",
    "    return model\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testing_hidden_data(hidden_data: pd.DataFrame) -> list:\n",
    "    '''DO NOT REMOVE THIS FUNCTION.\n",
    "\n",
    "The function accepts a dataframe as input and return an iterable (list)\n",
    "of binary classes as output.\n",
    "\n",
    "The function should be coded to test on hidden data\n",
    "and should include any preprocessing functions needed for your model to perform. \n",
    "    \n",
    "All relevant code MUST be included in this function.'''\n",
    "    # --- Preprocessing based on data analysis ---\n",
    "    hidden_data = preprocessing_data_analysis(hidden_data) # 42 features\n",
    "    \n",
    "    # ---  Preprocessing for model prediction ---\n",
    "    x_test_hidden_data = preprocessing_model(hidden_data) # 15 features\n",
    "\n",
    "    # ---  Load model and Predict the output ---\n",
    "    train_model = load_model(\"./train_model.pth\")\n",
    "    result = predict_output(x_test_hidden_data, train_model)\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Cell to check testing_hidden_data function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell should output a list of predictions.\n",
    "test_df = pd.read_parquet(filepath)\n",
    "test_df = test_df.drop(columns=[\"f_purchase_lh\"])\n",
    "print(testing_hidden_data(test_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Please have the filename renamed and ensure that it can be run with the requirements above being met. All the best!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
